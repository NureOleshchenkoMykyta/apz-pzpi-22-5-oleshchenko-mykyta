Міністерство освіти та науки України 

Харківський національний університет радіоелектроніки 

 

 

 

 

Лабораторна робота №4 

з дисципліни: «Архітектура програмного забезпечення» 

 

 

 

 

 

 

Виконав 

ст. гр. ПЗПІ-22-5 

Олещенко Микита Сергійович 

 

                                        

                                       Перевірив 

                                       Дашенков Д.С. 

 

 

 

Харків 2025 

 

 

Лабораторна робота №4 

 

Мета: 

Ознайомитись з підходами до масштабування бекенд-системи та реалізувати один з варіантів масштабування. Продемонструвати роботу системи при високому навантаженні та оцінити вплив масштабування на продуктивність системи. Провести навантажувальне тестування за допомогою спеціалізованого інструменту (Locust) і проаналізувати витримувану кількість запитів у залежності від кількості копій серверу.   

 

Хід роботи: 

Вибір стратегії масштабування  

Було обрано горизонтальне масштабування: кожен pod виконує однакову логіку бекенду. Масштабування відбувається за рахунок додавання нових копій сервера (реплік). Система розгорнута в Kubernetes Minikube-кластері з окремими подами для API сервера, бази даних та LoadBalancer-сервісом. 

Вивід команди kubectl get pods, яка показує, що всі необхідні поди (arkpz-api, postgres) знаходяться у статусі Running: 

Рисунок 

 

Результат виконання kubectl get services. Видно, що сервіс arkpz-api-service працює через LoadBalancer і має порт, за яким доступний API: 

 

Рисунок 

 

Команда minikube ip виводить IP-адресу, за якою доступний кластер ззовні. Вона використовується при тестуванні API (наприклад, http://192.168.49.2:30889): 

Рисунок 

 

Технічні рішення 

На початку роботи було встановлено та налаштовано середовище: Ubuntu з підтримкою WSL2, а також Docker (без Docker Desktop). Було створено Dockerfile для Flask API, зібрано образ, завантажено його до DockerHub. Для деплою у Kubernetes використано YAML-файли з описами сервісів і деплойментів. Сервер і база даних PostgreSQL працюють у окремих подах Minikube-кластера. 

 

Імпорт бази даних 

Імпортовано дамп бази даних у под PostgreSQL. У схемі analysisstate створено таблиці account, results, notes. Додано тестових користувачів. 

SQL-запит SELECT * FROM analysisstate.account;, який демонструє наявність тестових користувачів у базі даних PostgreSQL, зокрема example@gmail.com: 

Рисунок 

 

 

 

Перевірка працездатності авторизації  

Логін перевірено через curl POST-запит на /login. У відповідь система повертає JSON з повідомленням про успішний вхід. 

Вивід відповіді сервера на запит авторизації через команду curl, що підтверджує успішну роботу маршруту /login: 

Рисунок 

Так як JSON-рядки автоматично кодують не-ASCII символи у Unicode-escape-послідовності, то відповідь має такий вигляд. Тут повинно бути написано: 

 

{ 

“message”: “Вхід успішний”, 

“role”: “admin” 

} 

 

 

Навантажувальне тестування  

Для навантажувального тестування використано інструмент Locust.  

Суть тесту  

POST-запит на /login, що повторюється з кількох віртуальних користувачів.  

Параметри запуску  

50 користувачів, spawn rate — 5/sec, хост — http://192.168.49.2:30889  

Результати  

Через те, що таблиця результатів у Locust достатньо довга - я розділив її на 2 скриншоти: 

РисунокРисунок 

Інтерфейс Locust демонструє результати тестування при 50 одночасних користувачах, що надсилали POST-запити на /login. 

Загальна кількість запитів: 10 326 

Кількість помилок: 0 — жодного невдалого запиту. 

Середній час відповіді: 19 мс 

90-й перцентиль: 30 мс 

99-й перцентиль: 51 мс 

Максимальний час відповіді: 167 мс 

Поточне навантаження (RPS): 24.3 запити/сек 

Відсоток помилок: 0% 

Ці результати свідчать про стабільну роботу системи під навантаженням: часи відповіді залишаються низькими, а кількість запитів, які обробляються за секунду, є стабільно високою. Усі запити успішно оброблені.  

 

Аналіз вузьких місць  

Потенційним вузьким місцем є єдиний pod PostgreSQL. При масштабуванні навантаження зростатиме переважно на БД. У тесті система витримала навантаження без збоїв. 

 

 

Висновок 

У результаті виконання лабораторної роботи було успішно реалізовано горизонтальне масштабування бекенд-системи в середовищі Kubernetes. Для підготовки до роботи було встановлено Ubuntu та налаштовано Docker у WSL2. Бекенд-логіка була розгорнута у вигляді кількох подів, доступних через LoadBalancer, а база даних PostgreSQL працювала як окремий сервіс. 

  

Після налаштування середовища проведено навантажувальне тестування за допомогою Locust, яке показало: 

- понад 10 000 запитів до маршруту /login; 

- 0% помилок під час обробки запитів; 

- поточну пропускну здатність системи на рівні ~24.3 запити/сек; 

- середній час відповіді — 19 мс, що свідчить про ефективність масштабування. 

  

Тестування підтвердило стабільну роботу системи під високим одночасним навантаженням. Система готова до масштабування й здатна обробляти запити великої кількості користувачів. 

 

 

 

 

 

 

 

ДОДАТОК А 

Код для тестування 

from locust import HttpUser, task, between 

 

class WebsiteUser(HttpUser):  

     wait_time = between(1, 3)  

 

     @task 

     def login(self): 

     self.client.post(  

       "/login", 

  json={"email": "example@gmail.com", "password": "password123"}  

  ) 

 

 
